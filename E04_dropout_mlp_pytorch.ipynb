{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/staerkjoe/NLP_colab/blob/main/E04_dropout_mlp_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WJeMG1Akwvp"
      },
      "source": [
        "## Task 4.1: Theory and praxis of Regularization - Dropout\n",
        "\n",
        "ITU KSAMLDS1KU - Advanced Machine Learning for Natural Language Processing 2025\n",
        "\n",
        "by Stefan Heinrich, Eisuke Okuda, Jonathan Tiedchen,\n",
        "& material by Kevin Murphy and Chris Bishop.\n",
        "\n",
        "This notebook is based on http://d2l.ai/chapter_multilayer-perceptrons/dropout.html and further adaptations by Kevin Murphy in https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks/dropout_MLP_torch.ipynb\n",
        "\n",
        "All info and static material: https://learnit.itu.dk/course/view.php?id=3024752\n",
        "\n",
        "-------------------------------------------------------------------------------\n",
        "\n",
        "Note: An important difficulty in Deep Learning is the high dimensional dependency of hyper parameters. Often, finding a setting for a specific task is brittle, and it is not guaranteed that this setting works well on a (slightly) changed task. We, therefore, also test for different random seeds.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZyBF4-WksTv",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:30:30.118765500Z",
          "start_time": "2024-01-25T15:30:26.973219200Z"
        },
        "outputId": "97d4f7fe-6b41-4405-fb13-f49305624183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "# @title #### Import dependencies\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "np.random.seed(seed=0)\n",
        "\n",
        "!mkdir figures # for saving plots\n",
        "\n",
        "!wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/torch.py -q -O d2l.py\n",
        "import d2l\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2625438028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/torch.py -q -O d2l.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0md2l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/d2l.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdtd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0meurosat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEuroSAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfakedata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfer2013\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFER2013\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfgvc_aircraft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFGVCAircraft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/fakedata.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mautoaugment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_interpolation_modes_from_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Image_fromarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_functional_pil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_functional_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageEnhance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Image_fromarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# We search for the GPUs and otherwise use the CPU for the computation.\n",
        "# you can also set a fixed device here.\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "      return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')\n",
        "device = try_gpu()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-25T15:30:30.130861200Z",
          "start_time": "2024-01-25T15:30:30.114746600Z"
        },
        "id": "SZgKFYl3a2F2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzjmKaBXlcY8"
      },
      "source": [
        "#### Add dropout layer by hand to an MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY8jl-m7k_mM",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:30:30.151084400Z",
          "start_time": "2024-01-25T15:30:30.129863800Z"
        }
      },
      "source": [
        "def dropout_layer(X, dropout):\n",
        "    assert 0 <= dropout <= 1\n",
        "    # In this case, all elements are dropped out\n",
        "    if dropout == 1:\n",
        "        return torch.zeros_like(X)\n",
        "    # In this case, all elements are kept\n",
        "    if dropout == 0:\n",
        "        return X\n",
        "    mask = (torch.Tensor(X.shape).uniform_(0, 1) > dropout).float().to(device)\n",
        "    return mask * X / (1.0 - dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u62UJnwPlOj4",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:30:30.287869700Z",
          "start_time": "2024-01-25T15:30:30.147097300Z"
        }
      },
      "source": [
        "# quick test\n",
        "torch.manual_seed(0)\n",
        "X = torch.arange(16, dtype=torch.float32).reshape((2, 8))\n",
        "X = X.to(device)\n",
        "print(X)\n",
        "print(dropout_layer(X, 0.))\n",
        "print(dropout_layer(X, 0.5))\n",
        "print(dropout_layer(X, 1.))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzNntL4slPud",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:30:30.299057800Z",
          "start_time": "2024-01-25T15:30:30.283794400Z"
        }
      },
      "source": [
        "#  A common trend is to set a lower dropout probability closer to the input layer\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n",
        "                 is_training=True, dropout1=0.2, dropout2=0.5):\n",
        "        super(Net, self).__init__()\n",
        "        self.dropout1 = dropout1\n",
        "        self.dropout2 = dropout2\n",
        "        self.num_inputs = num_inputs\n",
        "        self.training = is_training\n",
        "        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n",
        "        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n",
        "        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n",
        "        # Use dropout only when training the model\n",
        "        if self.training == True:\n",
        "            # Add a dropout layer after the first fully connected layer\n",
        "            H1 = dropout_layer(H1, self.dropout1)\n",
        "        H2 = self.relu(self.lin2(H1))\n",
        "        if self.training == True:\n",
        "            # Add a dropout layer after the second fully connected layer\n",
        "            H2 = dropout_layer(H2, self.dropout2)\n",
        "        out = self.lin3(H2)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SV2d62Dlr28"
      },
      "source": [
        "#### Fit to FashionMNIST\n",
        "\n",
        "This time we use the [d2l.load_data_fashion_mnist](https://github.com/d2l-ai/d2l-en/blob/master/d2l/torch.py#L200) function to load our well-known FashionMNIST data set. For the purpose of understanding Dropout, a vision task is easier than an NLP task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Igif6rl3ci",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:30:30.376727600Z",
          "start_time": "2024-01-25T15:30:30.300085500Z"
        }
      },
      "source": [
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gajH4V3gmLVv"
      },
      "source": [
        "Fit model using SGD.\n",
        "Uses the [d2l.train_ch6](https://github.com/d2l-ai/d2l-en/blob/master/d2l/torch.py#L326) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrRopaCFlrJB"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# We pick a wide model to cause overfitting without dropout\n",
        "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
        "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n",
        "          dropout1=0.5, dropout2=0.5)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "lr = 0.5\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "num_epochs = 10\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgi3EjMrrc28"
      },
      "source": [
        "When we turn dropout off, we notice a slightly larger gap between train and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wd8I2-pn69g"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n",
        "          dropout1=0.0, dropout2=0.0)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "num_epochs = 10\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, device=device)\n",
        "#d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA3JYLAYojwU"
      },
      "source": [
        "#### Dropout using PyTorch layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmGh7Tf0olsK",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:45:10.162741100Z",
          "start_time": "2024-01-25T15:45:10.143502400Z"
        }
      },
      "source": [
        "dropout1 = 0.5\n",
        "dropout2 = 0.5\n",
        "net = nn.Sequential(\n",
        "    nn.Flatten(), nn.Linear(num_inputs, num_hiddens1), nn.ReLU(),\n",
        "    # Add a dropout layer after the first fully connected layer\n",
        "    nn.Dropout(dropout1), nn.Linear(num_hiddens2, num_hiddens1), nn.ReLU(),\n",
        "    # Add a dropout layer after the second fully connected layer\n",
        "    nn.Dropout(dropout2), nn.Linear(num_hiddens2, num_outputs))\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "net.apply(init_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYha_QFlorJr"
      },
      "source": [
        "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4_bZQYLs3G7"
      },
      "source": [
        "#### Visualize some predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kOtZU7xtErO",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:46:41.529808400Z",
          "start_time": "2024-01-25T15:46:41.511700900Z"
        }
      },
      "source": [
        "def display_predictions(net, test_iter, n=6):\n",
        "    # Extract first batch from iterator\n",
        "    for X, y in test_iter:\n",
        "        break\n",
        "    # Get labels\n",
        "    trues = d2l.get_fashion_mnist_labels(y)\n",
        "    preds = d2l.get_fashion_mnist_labels(d2l.argmax(net(X.to(device)), axis=1))\n",
        "    # Plot\n",
        "    titles = [true + '\\n' + pred for true, pred in zip(trues, preds)]\n",
        "    d2l.show_images(d2l.reshape(X[0:n], (n, 28, 28)), 1, n,\n",
        "                    titles=titles[0:n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE8DD-2Ys42Y",
        "ExecuteTime": {
          "end_time": "2024-01-25T15:46:45.520227500Z",
          "start_time": "2024-01-25T15:46:41.525722100Z"
        }
      },
      "source": [
        "display_predictions(net, test_iter)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}