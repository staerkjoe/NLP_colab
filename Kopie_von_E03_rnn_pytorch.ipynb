{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/staerkjoe/NLP_colab/blob/main/Kopie_von_E03_rnn_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCwSuMn6YVmk"
      },
      "source": [
        "## Task 3.1: Understanding Basic Recurrent Neural Networks (RNNs)\n",
        "\n",
        "ITU KSAMLNL1KU - Advanced Machine Learning for Natural Language Processing 2025\n",
        "\n",
        "by Stefan Heinrich, Eisuke Okuda, Jonathan Tiedchen,\n",
        "& material by Kevin Murphy and Chris Bishop.\n",
        "\n",
        "This notebook is based on sec 8.5 of http://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html with further contributions by Murphy.\n",
        "\n",
        "All info and static material: https://learnit.itu.dk/course/view.php?id=3024752\n",
        "\n",
        "-------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6VNlv_FYTbS",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.016244600Z",
          "start_time": "2023-11-28T13:24:37.750061400Z"
        }
      },
      "source": [
        "# @title #### Import dependencies\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from IPython import display\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import hashlib\n",
        "import time\n",
        "\n",
        "np.random.seed(seed=1)\n",
        "torch.manual_seed(1)\n",
        "!mkdir figures # for saving plots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiYH4kGgak5S"
      },
      "source": [
        "#### Load the data\n",
        "\n",
        "As data, we use the book \"The Time Machine\" by H G Wells,\n",
        "preprocessed using the code in [this colab](https://github.com/probml/pyprobml/blob/master/notebooks/tutorials/text_preproc_torch.ipynb).\n",
        "Don't spend much time on understanding the details of the preprocessing but grasp the overall concept of how the **vocabulary is built** and what the **role of \"num_steps\"** is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRp-MH0Nv7rN",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.033080400Z",
          "start_time": "2023-11-28T13:24:40.020225300Z"
        }
      },
      "source": [
        "def download(name, cache_dir=os.path.join('..', 'data')):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, 'rb') as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f'Downloading {fname} from {url}...')\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download('time_machine'), 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, 'char')\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "def load_data_time_machine(batch_size, num_steps, use_random_iter=False,\n",
        "                           max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,\n",
        "                              max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TvZsL4gtHQ2",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.063036700Z",
          "start_time": "2023-11-28T13:24:40.036065600Z"
        }
      },
      "source": [
        "def tokenize(lines, token='word'):\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "    # sequence\n",
        "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "    # Subtract 1 since we need to account for labels\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    # The starting indices for subsequences of length `num_steps`\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    # In random sampling, the subsequences from two adjacent random\n",
        "    # minibatches during iteration are not necessarily adjacent on the\n",
        "    # original sequence\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\n",
        "        return corpus[pos:pos + num_steps]\n",
        "\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        # Here, `initial_indices` contains randomized starting indices for\n",
        "        # subsequences\n",
        "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = torch.tensor(corpus[offset:offset + num_tokens])\n",
        "    Ys = torch.tensor(corpus[offset + 1:offset + 1 + num_tokens])\n",
        "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_steps * num_batches, num_steps):\n",
        "        X = Xs[:, i:i + num_steps]\n",
        "        Y = Ys[:, i:i + num_steps]\n",
        "        yield X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLIKRJomvBV5",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.066022500Z",
          "start_time": "2023-11-28T13:24:40.048082600Z"
        }
      },
      "source": [
        "class SeqDataLoader:\n",
        "    \"\"\"An iterator to load sequence data.\"\"\"\n",
        "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "        if use_random_iter:\n",
        "            self.data_iter_fn = seq_data_iter_random\n",
        "        else:\n",
        "            self.data_iter_fn = seq_data_iter_sequential\n",
        "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
        "\n",
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
        "        uniq_tokens += [\n",
        "            token for token, freq in self.token_freqs\n",
        "            if freq >= min_freq and token not in uniq_tokens]\n",
        "        self.idx_to_token, self.token_to_idx = [], dict()\n",
        "        for token in uniq_tokens:\n",
        "            self.idx_to_token.append(token)\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG2T4maNYdsG",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.144191Z",
          "start_time": "2023-11-28T13:24:40.064032900Z"
        }
      },
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
        "DATA_HUB['time_machine'] = (DATA_URL + 'timemachine.txt',\n",
        "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "\n",
        "# load data as function (thus, will be executed later!)\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Inspect some of the data"
      ],
      "metadata": {
        "id": "scpZP3kdI2wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print lines 0, 8 and 11 of the original text, after it was stripped from standard non-characters:\n",
        "X = read_time_machine()\n",
        "print(X[0])\n",
        "print(X[8])\n",
        "print(X[11])\n",
        "# Note: this stripping, as a pre-processing step, can already remove important cues for meaning."
      ],
      "metadata": {
        "id": "scsJFyNyHw0C",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.204432400Z",
          "start_time": "2023-11-28T13:24:40.142791400Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQryRwZauLq"
      },
      "source": [
        "### Build the model - Variant 1: RNN from scratch\n",
        "\n",
        "We fit an unconditional RNN, for language modeling (ie. not vec2seq or seq2seq. Following the D2L notation, the model has the form\n",
        "$$\\begin{align}\n",
        "H_t &= \\phi(X_t W_{xh} + H_{t-1}  W_{hh} + b_h) \\\\\n",
        "O_t &= H_t W_{hq} + b_q\n",
        "\\end{align}\n",
        "$$\n",
        "where $X_t$ is the $(n,d)$ matrix of (one-hot) inputs\n",
        "(for batch size $n$ and vocabulary size $d$),\n",
        "$H_t$ is the $(n,h)$ matrix of hidden states\n",
        "(for $h$ hidden states),\n",
        "and $O_t$ is the $(n,q)$ matrix of output logits\n",
        "(for $q$ output labels, often $q=d$).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TC24W0EanF7",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.216392300Z",
          "start_time": "2023-11-28T13:24:40.174464Z"
        }
      },
      "source": [
        "# Create the initial parameters\n",
        "\n",
        "def get_params(vocab_size, num_hiddens, device):\n",
        "    num_inputs = num_outputs = vocab_size\n",
        "\n",
        "    def normal(shape):\n",
        "        return torch.randn(size=shape, device=device) * 0.01\n",
        "\n",
        "    # Hidden layer parameters\n",
        "    W_xh = normal((num_inputs, num_hiddens))\n",
        "    W_hh = normal((num_hiddens, num_hiddens))\n",
        "    b_h = torch.zeros(num_hiddens, device=device)\n",
        "    # Output layer parameters\n",
        "    W_hq = normal((num_hiddens, num_outputs))\n",
        "    b_q = torch.zeros(num_outputs, device=device)\n",
        "    # Attach gradients\n",
        "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
        "    for param in params:\n",
        "        param.requires_grad_(True)\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEf08b5ccaPx",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.217393Z",
          "start_time": "2023-11-28T13:24:40.190410500Z"
        }
      },
      "source": [
        "# Create the initial state\n",
        "# We assume this is a tuple of one element (later we will use longer tuples)\n",
        "def init_rnn_state(batch_size, num_hiddens, device):\n",
        "    return (torch.zeros((batch_size, num_hiddens), device=device),)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkvId2WmcqFz",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.231565400Z",
          "start_time": "2023-11-28T13:24:40.207421700Z"
        }
      },
      "source": [
        "# Forward function.\n",
        "# Input sequence is (T,B,V), where T is length of the sequence, B is batch size, V is vocab size.\n",
        "# We iterate over each time step, and process the batch (for that timestep in parallel).\n",
        "# Output sequence is (T*B, V), since concatante all the time steps.\n",
        "# We also return the final state, so we can process the next subsequence.\n",
        "\n",
        "def rnn(inputs, state, params):\n",
        "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "    H, = state\n",
        "    outputs = []\n",
        "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "    for X in inputs:\n",
        "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
        "        Y = torch.mm(H, W_hq) + b_q\n",
        "        outputs.append(Y)\n",
        "    return torch.cat(outputs, dim=0), (H,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw3vhoRKdVxt",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.252564400Z",
          "start_time": "2023-11-28T13:24:40.224584900Z"
        }
      },
      "source": [
        "# Make the model class\n",
        "# Input X to call is (B,T) matrix of integers (from vocab encoding).\n",
        "# We transpse this to (T,B) then one-hot encode to (T,B,V), where V is vocab.\n",
        "# The result is passed to the forward function.\n",
        "# (We define the forward function as an argument, so we can change it later.)\n",
        "\n",
        "class RNNModelScratch:\n",
        "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, device, get_params,\n",
        "                 init_state, forward_fn):\n",
        "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "        self.params = get_params(vocab_size, num_hiddens, device)\n",
        "        self.init_state, self.forward_fn = init_state, forward_fn\n",
        "\n",
        "    def __call__(self, X, state):\n",
        "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
        "        return self.forward_fn(X, state, self.params)\n",
        "\n",
        "    def begin_state(self, batch_size, device):\n",
        "        return self.init_state(batch_size, self.num_hiddens, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY_gAWKLw0vR",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.257549Z",
          "start_time": "2023-11-28T13:24:40.237613800Z"
        }
      },
      "source": [
        "# We search for the GPUs and otherwise use the CPU for the computation.\n",
        "# you can also set a fixed device here.\n",
        "def try_gpu(i=0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "      return torch.device(f'cuda:{i}')\n",
        "    return torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2brHhpdbh5",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.306476600Z",
          "start_time": "2023-11-28T13:24:40.256551400Z"
        }
      },
      "source": [
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
        "                      init_rnn_state, rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibw5knTqdxrv",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.379982100Z",
          "start_time": "2023-11-28T13:24:40.301969600Z"
        }
      },
      "source": [
        "X = torch.arange(10).reshape((2, 5)) # batch 2, sequence length is 5\n",
        "state = net.begin_state(X.shape[0], try_gpu())\n",
        "print(len(state)) # length 1\n",
        "print(state[0].shape) # (2,512)\n",
        "\n",
        "Y, new_state = net(X.to(try_gpu()), state)\n",
        "print(len(vocab)) # 28\n",
        "print(Y.shape) # (2x5, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLtX5YfSfiBn"
      },
      "source": [
        "#### The prediction (generation) task\n",
        "\n",
        "We pass in an initial prefix string, that is not generated. This is used to \"warm-up\" the hidden state. Specifically, we update the hidden state given the observed prefix, but don't generate anything. After that, for each of the T steps, we compute the (1,V) output tensor, pick the argmax index, and append it to the output. Finally, we convert the to indices to readable token sequence of size (1,T). (Note that this is a greedy, deterministic procedure.)\n",
        "\n",
        "(D2L calls this `predict_ch8` since it occurs in their chapter 8.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvGhZUIGd3F3",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.423668900Z",
          "start_time": "2023-11-28T13:24:40.380977300Z"
        }
      },
      "source": [
        "def predict(prefix, num_preds, net, vocab, device):\n",
        "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "    state = net.begin_state(batch_size=1, device=device)\n",
        "    outputs = [vocab[prefix[0]]]\n",
        "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(\n",
        "        (1, 1))\n",
        "    for y in prefix[1:]:  # Warm-up period\n",
        "        _, state = net(get_input(), state)\n",
        "        outputs.append(vocab[y])\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "        y, state = net(get_input(), state)\n",
        "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
        "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "678bHt1ogoh8",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.435629300Z",
          "start_time": "2023-11-28T13:24:40.393769Z"
        }
      },
      "source": [
        "# sample 50 characters after the prefix.\n",
        "# since the model is untrained, the results will be garbage.\n",
        "predict('time traveller ', 50, net, vocab, try_gpu())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WTrDGSIhQxN"
      },
      "source": [
        "#### Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPuo8za_hUWL"
      },
      "source": [
        "To ensure the gradient doesn't blow up when doing backpropagation through many layers, we use gradient clipping, which corresponds to the update\n",
        "$$\n",
        "g := \\min(1, \\theta /||g||) g\n",
        "$$\n",
        "where $\\theta$ is the scaling parameter, and $g$ is the gradient vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDuvumDhgpvh",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.470445Z",
          "start_time": "2023-11-28T13:24:40.425662500Z"
        }
      },
      "source": [
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Exq_Lihxld"
      },
      "source": [
        "The training step is fairly standard, except for the use of gradient clipping, and the issue of the hidden state.\n",
        "If the data iterator uses random ordering of the sequences, we need to initialize the hidden state for each minibatch. However, if the data iterator uses sequential ordering, we only initialize the hidden state at the very beginning of the process. In the latter case, the hidden state will depend on the value at the previous minibatch. We detach the state vector to prevent gradients flowing across minibatch boundaries.\n",
        "\n",
        "The state vector may be a tensor or a tuple, depending on what kind of RNN we are using. In addition, the parameter updater can be a built-in optimizer, or the simpler D2L sgd optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Utility functions\n",
        "*Note*: Ignore the methods in this section. The code is not necessary to unterstand for the tasks. They are just used for some nicer visualisations."
      ],
      "metadata": {
        "id": "gPDPDzZLlMpk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA_QeZH5rp1o",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.518398800Z",
          "start_time": "2023-11-28T13:24:40.450192100Z"
        }
      },
      "source": [
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        #deprecated: display.set_matplotlib_formats('svg')\n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes,]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: set_axes(self.axes[\n",
        "            0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return np.array(self.times).cumsum().tolist()\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()\n",
        "\n",
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        for param in params:\n",
        "            param -= lr * param.grad / batch_size\n",
        "            param.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training loop (over epoch)"
      ],
      "metadata": {
        "id": "0PtMgbDzlZll"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq_QMLUKhw3k",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.528365600Z",
          "start_time": "2023-11-28T13:24:40.457490400Z"
        }
      },
      "source": [
        "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
        "    state, timer = None, Timer()\n",
        "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    for X, Y in train_iter:\n",
        "        if state is None or use_random_iter:\n",
        "            # Initialize `state` when either it is the first iteration or\n",
        "            # using random sampling\n",
        "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "        else:\n",
        "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "                # `state` is a tensor for `nn.GRU`\n",
        "                state.detach_()\n",
        "            else:\n",
        "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
        "                # for our custom scratch implementation\n",
        "                for s in state:\n",
        "                    s.detach_()\n",
        "        y = Y.T.reshape(-1) #(B,T) -> (T,B)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat, state = net(X, state)\n",
        "        l = loss(y_hat, y.long()).mean()\n",
        "        if isinstance(updater, torch.optim.Optimizer):\n",
        "            updater.zero_grad()\n",
        "            l.backward()\n",
        "            grad_clipping(net, 1)\n",
        "            updater.step()\n",
        "        else:\n",
        "            l.backward()\n",
        "            grad_clipping(net, 1)\n",
        "            # batch_size=1 since the `mean` function has been invoked\n",
        "            updater(batch_size=1)\n",
        "        metric.add(l * y.numel(), y.numel())\n",
        "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T64jNH0Ciq4o"
      },
      "source": [
        "The main training function is fairly standard.\n",
        "The loss function is per-symbol cross-entropy, $-\\log q(x_t)$, where $q$ is the model prediction from the RNN. Since we compute the average loss across time steps within a batch, we are computing $-\\frac{1}{T} \\sum_{t=1}^T \\log p(x_t|x_{1:t-1})$. The exponential of this is the perplexity (ppl). We plot this metric during training, since it is independent of document length. In addition, we print the MAP sequence prediction following the suffix 'time traveller', to get a sense of what the model is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tPGxQDWiqfl",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:24:40.529362Z",
          "start_time": "2023-11-28T13:24:40.472440Z"
        }
      },
      "source": [
        "def train(net, train_iter, vocab, lr, num_epochs, device,\n",
        "              use_random_iter=False):\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    animator = Animator(xlabel='epoch', ylabel='perplexity',\n",
        "                            legend=['train'], xlim=[10, num_epochs])\n",
        "    # Initialize\n",
        "    if isinstance(net, nn.Module):\n",
        "        updater = torch.optim.SGD(net.parameters(), lr)\n",
        "    else:\n",
        "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
        "    num_preds = 50\n",
        "    predict_ = lambda prefix: predict(prefix, num_preds, net, vocab, device)\n",
        "    # Train and predict\n",
        "    for epoch in range(num_epochs):\n",
        "        ppl, speed = train_epoch(net, train_iter, loss, updater, device,\n",
        "                                     use_random_iter)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            # sample again after the prefix (live during training).\n",
        "            # the result should be better now.\n",
        "            print(predict_('time traveller '))\n",
        "            animator.add(epoch + 1, [ppl])\n",
        "    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n",
        "\n",
        "    # sample again after the prefix (live during training).\n",
        "    # the result should be better now.\n",
        "    print(predict_('time traveller '))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfD7J9-YjSCi"
      },
      "source": [
        "num_epochs, lr = 500, 1\n",
        "device = try_gpu()\n",
        "torch.manual_seed(42)\n",
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
        "                      init_rnn_state, rnn)\n",
        "train(net, train_iter, vocab, lr, num_epochs, try_gpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis\n",
        "Inspect some more results:"
      ],
      "metadata": {
        "id": "gYyfwxNw_dRb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ3nTS2flVtH",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:26:03.642432900Z",
          "start_time": "2023-11-28T13:26:03.437111800Z"
        }
      },
      "source": [
        "num_preds = 400\n",
        "predict_ = lambda prefix: predict(prefix, num_preds, net, vocab, device)\n",
        "\n",
        "print(predict_('time traveller '))\n",
        "print(predict_('the '))\n",
        "print(predict_('fire burnt brightly '))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URvl3gPSvy2P"
      },
      "source": [
        "### Build the model - Variant 2: Creating a PyTorch module\n",
        "\n",
        "We now show how to use create an RNN as a module, which is faster than our pure Python implementation.\n",
        "\n",
        "First we create a single hidden chain to represent the state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cbe7sYAsAjr",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:26:03.652638900Z",
          "start_time": "2023-11-28T13:26:03.635456300Z"
        }
      },
      "source": [
        "num_hiddens = 256\n",
        "rnn_layer = nn.RNN(len(vocab), num_hiddens)\n",
        "\n",
        "batch_size, num_steps = 32, 35\n",
        "num_layers = 1\n",
        "state = torch.zeros((num_layers, batch_size, num_hiddens))\n",
        "state.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1hVhsSgwLLU"
      },
      "source": [
        "Now we update the state with a random one-hot tensor of inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pAV6m9rwN1d",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:26:03.713572400Z",
          "start_time": "2023-11-28T13:26:03.650644700Z"
        }
      },
      "source": [
        "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
        "Y, state_new = rnn_layer(X, state)\n",
        "Y.shape, state_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J9R2aHzwhK_"
      },
      "source": [
        "Now we make an RNN module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89IqhaoSwiRT",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:26:03.714569400Z",
          "start_time": "2023-11-28T13:26:03.686161600Z"
        }
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"The RNN model.\"\"\"\n",
        "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "        super(RNNModel, self).__init__(**kwargs)\n",
        "        self.rnn = rnn_layer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_hiddens = self.rnn.hidden_size\n",
        "        # If the RNN is bidirectional (to be introduced later),\n",
        "        # `num_directions` should be 2, else it should be 1.\n",
        "        if not self.rnn.bidirectional:\n",
        "            self.num_directions = 1\n",
        "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "        else:\n",
        "            self.num_directions = 2\n",
        "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "\n",
        "    def forward(self, inputs, state):\n",
        "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
        "        X = X.to(torch.float32)\n",
        "        Y, state = self.rnn(X, state)\n",
        "        # The fully connected layer will first change the shape of `Y` to\n",
        "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
        "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
        "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "        return output, state\n",
        "\n",
        "    def begin_state(self, device, batch_size=1):\n",
        "        if not isinstance(self.rnn, nn.LSTM):\n",
        "            # `nn.GRU` takes a tensor as hidden state\n",
        "            return torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                                batch_size, self.num_hiddens), device=device)\n",
        "        else:\n",
        "            # `nn.LSTM` takes a tuple of hidden states\n",
        "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                                 batch_size, self.num_hiddens),\n",
        "                                device=device),\n",
        "                    torch.zeros((self.num_directions * self.rnn.num_layers,\n",
        "                                 batch_size, self.num_hiddens),\n",
        "                                device=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVO0IVrGxBD1"
      },
      "source": [
        "Test the untrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J19s-kDjwi3F",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:26:03.717592100Z",
          "start_time": "2023-11-28T13:26:03.699788100Z"
        }
      },
      "source": [
        "device = try_gpu()\n",
        "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
        "net = net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6RGAFRrxIMl"
      },
      "source": [
        "Train it. The results are similar to the 'from scratch' implementation, but much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_mu4sc2xEQH"
      },
      "source": [
        "num_epochs, lr = 500, 1\n",
        "train(net, train_iter, vocab, lr, num_epochs, device)\n",
        "predict('time traveller ', 50, net, vocab, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis\n",
        "Inspect some more results again."
      ],
      "metadata": {
        "id": "pt5YRaJN_lIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_preds = 400\n",
        "predict_ = lambda prefix: predict(prefix, num_preds, net, vocab, device)\n",
        "\n",
        "print(predict_('time traveller '))\n",
        "print(predict_('the '))\n",
        "print(predict_('fire burnt brightly '))\n"
      ],
      "metadata": {
        "id": "ktf_42uG_C9D",
        "ExecuteTime": {
          "end_time": "2023-11-28T13:26:47.501970800Z",
          "start_time": "2023-11-28T13:26:47.218899100Z"
        }
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}